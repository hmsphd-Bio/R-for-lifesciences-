---
title: "Linear regression practice"
output:
  html_document:
    df_print: paged
---

**Linear regression** is

a statistical and machine learning method to model the relationship between a dependent variable and one or more independent variables by fitting the data to a straight line (or plane/hyperplane) to make predictions, finding the line that minimizes errors between predicted and actual values, using the equation

[ùëå=ùëé+bX for simple cases where ùëå (Y\^cap) is predicted from X.]{.underline}

$$\hat{Y}=a+bX$$It's fundamental for understanding how changes in input variables (predictors) affect an output (target) and is used for forecasting, like predicting sales from advertising spend or performance from study hours.

In this markdown we are going to make a Linear regression model using the inbuilt *mtcars* sample dataset in R studio:-

First, let's load the packages that will be needed

```{r}
library(tidyverse)
library(equatiomatic)
```

```{r}
mtcars
```

Our dataset has many variables. **Let's create a linear model to understand how Weight (*wt*) of the vehicle affects the milage (*mpg*):**

-   In R, the Response variable (a.k.a the dependent variable) is written on the left side of the formula sign '\~'. The Predictor variables(s) (a.k.a Independent variables, such as weight and/or horsepower) is/are written on the right of the '\~' sign:

-   In our case we are using just a single predictor variable (weight), to predict the response variable (milage). The resulting formula, called the ***Linear function***, is denoted as:

    $\hat{Y} = {a+bX}$

-   Where,

    -   $a$= **Intercept** of the regression line at Y-axis

    -   $b$ = **slope** for variable X

-   The linear model can be created using the `lm()` function in R.

```{r}
mtcars_lm <- lm(mpg ~ wt,mtcars) # Formula for linear regression is [y= a+ bx]
```

Once the linear model is created, the statistical parameters of the model can be called using the

`summary()` function.

```{r}
summary(mtcars_lm)
```

The accuracy of a regression model is gauged by two main output parameters:

-   Standard error (S.E)
-   adjusted $R^2$ value

\- A model with higher adj$R^2$ and lower **S.E** is considered superior.

\- Both the parameters are determined by the number of predictor varibles fed in the model, their exponents (if any) ; as well as the values of the variables. \n

The equation (Linear function) corresponding to our model can be printed via `equatiomatic::preview_eq()`

function. The output will be a LaTeX formatted equation displayed in the Viewer pane.

```{r}
preview_eq(mtcars_lm)
```

**Linear regression relies on key assumptions** for valid results:¬†a¬†**linear relationship**¬†between variables,¬†**independent errors**,¬†**homoscedasticity**¬†(constant error variance),¬†**normality of residuals**, and¬†**no multicollinearity**¬†among predictors, ensuring reliable predictions and coefficient interpretation. Violations can lead to biased estimates, making these assumptions crucial for accurate analysis.

A model must fulfill these assumptions to make satitistically sound and overall accurate predictions of the response variable (Milage).

**Model assesment and diagnosis**: To assess the fit of the datapoints (of our selected variables) in the model, and to test whether the model deviates from any of the pre-requisite assumptions of linear regression,the `plot()` function in R allows us to create 4 main *Diagnostic plots,* which provide a visual summary of the fit of various aspects of the model:

1)  ) `Residuals v/s fitted values plot:`
    -   Checks the assumption of linearity and homoscedasity

    -   The plot displays the ***residuals*** *(the differences between observed and predicted values)* on the y-axis against the ***fitted*** *(predicted)* values on the x-axis.

    -   For a good linear model, the points should be randomly scattered around the horizontal dashed line at zero, forming a horizontal band with no distinct pattern.

    -   The plot in the image shows a distinct¬†**U-shaped or curved red line**¬†(a locally weighted scatterplot smoothing line, or¬†`lowess`¬†line, fitted to the residuals), which deviates significantly from the horizontal zero line. This pattern indicates a violation of the¬†**linearity assumption**, suggesting that a simple linear model is not appropriate for the data and a non-linear relationship (e.g., a quadratic term) might be better suited.
2)  ) `Normal Q-Q (Quantile-Quantile) Plot:`
    -   This plot is a diagnostic tool used to assess whether the¬†**residuals are normally distributed.**

    -   The Q-Q plot compares the distribution of the standardized residuals (y-axis, "Standardized residuals") against an ideal normal distribution (x-axis, "Theoretical Quantiles").

    -   If the residuals are perfectly normally distributed, the points should lie exactly along the dashed diagonal line

    -   The points in our plot generally follow the line, but there are significant deviations, particularly at the tails (the high and low ends). The points at the top right, corresponding to the "Chrysler Imperial" and "Fiat 128" vehicles, curve above the line, indicating that the observed residuals in the upper tail are larger than expected in a perfect normal distribution.

    -   This pattern suggests a violation of the¬†**normality assumption**¬†for the residuals, which means the model's predictions might not be reliable, especially for extreme values
3)  ) **`Scale-Location plot`**`(also known as a Spread-Location plot):`
    -   This plot is primarily used to assess the regression assumption of¬†**homoscedasticity**¬†(constant variance of residuals)

    -   The plot displays the fitted (predicted) values on the x-axis and the square root of the standardized residuals ‚àöùëÜùë°ùëéùëõùëëùëéùëüùëëùëñùëßùëíùëëùëÖùëíùë†ùëñùëëùë¢ùëéùëôùë† on the y-axis. Taking the square root helps to spread out the points and makes patterns of non-constant variance (heteroscedasticity) easier to spot.

    -   ¬†In an ideal model with constant variance, the points should form a horizontal band of randomly scattered points, and the red smooth line (a¬†`lowess`¬†line) should be roughly horizontal and close to zero.

    -   In this specific plot, the red line is¬†**not horizontal**; it curves downward in the middle. Furthermore, the vertical spread of the points changes across the range of fitted values. This non-random pattern indicates a violation of the homoscedasticity assumption, meaning the variability of the residuals is not constant across all predicted values.
4)  ) `Residuals vs Leverage plot:`
    -   identifes¬†**influential observations**‚Äîdata points that have a disproportionate impact on the regression model's coefficients.

    -   Ideally, Most points should be clustered near the center of the plot with low leverage

        and small residuals.

    -   The **Chrysler Imperial** is located near the top right, indicating it has very high leverage (high weight relative to others) and a large positive residual. It sits close to or crosses the 0.5 Cook's distance line, making it a primary influential observation.

    -   The presence of points like the Chrysler Imperial with a large Cook's distance suggests that removing or adjusting this single data point would significantly change the parameters of the¬†`lm(mpg ~ wt)`¬†regression model.

```{r}
plot(mtcars_lm)
```

Since our diagnostic plots are suggesting that the datapoints of our model are showing a non-linear distribution (in addition to having a few highly influential values, such as 'Chrysler Imperial'), the dataset can be improved by :

1.) Removing the highly influential values (optional and not always recommended).

2.) Try and fit a polynomial (exponent of predictor variable) dataset in the model, to account for any non-linearity in the data.

Let's try removing the most influential datapoints (Chrysler Imperial,etc.) from the dataset and see how do the $R^2$ and S.E change.

```{r}
mtcars_modified <- mtcars %>%
  filter(! rownames(.) %in% c('Chrysler Imperial','Fiat 128','Toyota Corolla'))

mtcars_modified

dim(mtcars_modified)

```

```{r}
mtcars_lm_2 <- lm(mpg~wt,mtcars_modified)
summary(mtcars_lm_2)
```

Removing the influential value did improve the the $R^2$ and **S.E.**

However, since the weight of the Chrysler Imperial (and other two models) is a true measure (i.e less likely to be arbitrary and vague) it is best to keep such value in the dataset to make our model more robust for any similar outliers that it will encounter during the testing phase.

Therefore, we will not use the modified dataset but instead we will be using the original mtcars data based linear model, in the subsequent analysis in this script, despite its lower $R^2$ and inflated S.E.

```{r}
plot(mtcars_lm_2)
```

The diagnostic plots of the modified linear model do report some improvement in the fit of the data points , however, the non-linearity in the distribution of the data points still needs to be addressed.

Therefore, let's use our original `mtcars` data (instead of the modified `mtcars` data) and try to

address the non-linearity (and thereby improve the model) by incorporating exponent values of the predictor variable (wt) in our regression model.

Let's start by a second degree ($wt^2$) polynomial regression model.

```{r}
Poly_reg_mtcars <- lm(mpg~poly(wt,2),mtcars)  # formula for polynomial regression of 2nd degree is = > [y = y' + ax + b(x^2)] 
summary(Poly_reg_mtcars)
```

Our second degree polynomial regression model is significantly better and more accurate than its linear predecessor, as evident by the increased $R^2$ and a deflated S.E.!

Additionaly, it is important to know how does the linear function (equation) of a polynomial linear regression model look like:

```{r}
preview_eq(Poly_reg_mtcars)
```

In case if you're curious about how the polynomial model made with the modified mtcars would've looked like (only for demonstration):

```{r}
Poly_reg_mtcars_modified <- lm(mpg~poly(wt,2),mtcars_modified) 
summary(Poly_reg_mtcars_modified)
```

Again, It is best not use a model based on the modified `mtcars` dataset (and, as mentioned before, we will not be dong so! ).

Now, let's see whether increasing the exponent of the wt variable to third degree($wt^3$) will further improve our model or not?

```{r}
summary(lm(mpg~poly(wt,3),mtcars))
```

Increasing the exponent of wt variable to third degree, made the model worse (evident by its lower $R^2$ and inflated S.E,compared to the 2nd degree polynomial model). We'll therefore stick to the 2nd degree polynomial model.

It is a common practise to plot the relationship between the Response and predictor variable(s).

Let's use ggplot2 to graph this relationship:

1.) Linear relationship (mpg\~wt)

```{r}
library(ggpubr)
ggplot(mtcars,aes(wt,mpg))+
  geom_point()+
  geom_smooth(method='lm',se=TRUE)+ #by default the formula used is y=a+bx
  stat_regline_equation(aes(label = after_stat(eq.label)),label.x = 4.5,label.y = 25) + # Adds the linear equation (linear function)
  stat_cor(aes(label = after_stat(rr.label)), label.y = 38,label.x = 2.05 , color = "red") + #Adds the R^2 value
  theme_minimal()+
  coord_cartesian(ylim=c(0,40)) 
```

The same plot with marginal plots:

```{r}
library(ggExtra)
p <-ggplot(mtcars,aes(wt,mpg))+
  geom_point()+
  geom_smooth(method='lm',se=TRUE)+
  stat_regline_equation(aes(label = after_stat(eq.label)),label.x = 4.5,label.y = 25) + # Adds the linear equation (linear function)
  stat_cor(aes(label = after_stat(rr.label)), label.y = 38,label.x = 2.05 , color = "red") + #Adds the R^2 value
  theme_minimal()+
  coord_cartesian(ylim=c(0,40)) 

ggMarginal(p, type = "density", fill = "skyblue", col = "gray")
```

2.) The quadratic relationship (polynomial) can also be visualized (mpg\~$wt^2$)

-   Make sure that the quadratic equation is explicity specified in the 'formula=' parameter of the `geom_smooth()` function.

```{r}
ggplot(mtcars, aes(wt,mpg))+
  geom_point()+
  geom_smooth(method = 'lm',formula = y~poly(x,2), color= 'yellow')+ #To make a polynomial regression model out of the data
  stat_regline_equation(
    aes(label =  paste(after_stat(eq.label), after_stat(rr.label), sep = "~~~~~")),
    formula = y ~ poly(x, 2), 
    label.x = 3.5, label.y = 30)+ #Adds the R^2 value as well as the linear function equation of the polynomial regression model
  theme_minimal()
```

Optionally, I'm also plotting the graph for modified `mtcars` dataset

```{r}
ggplot(mtcars_modified,aes(wt,mpg))+
  geom_point()+
  geom_smooth(method='lm',se=TRUE)+
  stat_regline_equation(aes(label = after_stat(eq.label)),label.x = 4.5,label.y = 25) + # Adds the linear equation (linear function)
  stat_cor(aes(label = after_stat(rr.label)), label.y = 38,label.x = 4.05 , color = "red") + #Adds the R^2 value
  theme_minimal()+
  coord_cartesian(ylim=c(0,40)) 
```

The graph shows a strong negative correlationv(a negative slope value) between mpg and wt (which is expected).

```{r}
ggplot(mtcars_modified,aes(wt,mpg))+
  geom_point()+
  geom_smooth(method = 'lm',formula = y~poly(x,2), color= 'yellow')+ #To make a polynomial regression model out of the data
  stat_regline_equation(aes(label =  paste(after_stat(eq.label), after_stat(rr.label), sep = "~~~~~")),
    formula = y ~ poly(x, 2), 
    label.x = 3.5, label.y = 30)+ #Adds the R^2 value as well as the linear function equation of the polynomial regression model
  theme_minimal()
```

```{r}
cooks.distance(lm(mpg~poly(wt,2),mtcars_modified)) %>% sort(.,decreasing=TRUE) %>% as.data.frame()
```

R allows us to isolate specific attributes of the regression model via simple vector indexing '\$' :

```{r}
mtcars_lm$coefficients
Poly_reg_mtcars$coefficients

coefficients(mtcars_lm) # Alternatively, instead of the indexing,Specific function can also used. eg. `coefficients()` to isolate coefficients.



mtcars_lm$residuals
Poly_reg_mtcars$residuals

residuals(mtcars_lm) # to isolate residuals. Please read about residuals.



mtcars_lm$effects
Poly_reg_mtcars$effects



mtcars_lm$fitted.values
Poly_reg_mtcars$fitted.values

fitted.values(mtcars_lm) # to isolate fitted values (predicted values)


```

```{r}
predict(mtcars_lm)# `predict()` also isolates the fitted values.
predict(Poly_reg_mtcars)
```

```{r}
mtcars_lm$rank

mtcars_lm$assign
```

We can isolate different statistical parameters and aspects of the model into a vector or table form to make insightful plots and draw valuable conclusions about the model (we'll see how, later in this script).

**Now, let's move forward and make a regression model which is based on two Predictor variables:**

```{r}
New_mtcars_lm <- lm(mpg~ wt+gear, mtcars) # Combination no 1 -> 0.77 R^2 +  S.E = 5.09
summary(New_mtcars_lm)
```

```{r}

New_mtcars_lm <- lm(mpg~ wt+disp, mtcars) # Combination no 1 -> 0.765 R^2 + S.E = 2.164
summary(New_mtcars_lm)
```

```{r}
New_mtcars_lm <- lm(mpg~ wt+cyl, mtcars) # Combination no :2 -> 0.818 R^2+  S.E = 1.715
summary(New_mtcars_lm)
```

```{r}
New_mtcars_lm <- lm(mpg~ wt+hp, mtcars) # Combination no :3 -> 0.814 R^2 +  S.E= 1.59
summary(New_mtcars_lm)
```

```{r}
New_mtcars_lm <- lm(mpg~ wt+vs, mtcars) # Combination no :4 -> 0.787 R^2 &  2.355 S.E
summary(New_mtcars_lm)
```

```{r}
New_mtcars_lm <- lm(mpg~ wt+carb, mtcars) # Combination no :5 -> 0.778 R^2 & 1.76 S.E
summary(New_mtcars_lm)
```

Based on the $R^2$ and S.E values of the above **bivariate** combinations; wt, hp, and cyl variables explain the variance in mpg much more precisely than other variables in the dataset.

It is rational to create a **trivariate model** using 'wt', 'cyl', and 'hp' and see how well it performs compared to our previous models.

```{r}
New_mtcars_lm <- lm(mpg~ wt+cyl+hp, mtcars) # Combination no :6 -> a Multivariate (3 predictor variables) linear model, with a high 0.826 adj-R^2 & a low 1.78 S.E
summary(New_mtcars_lm)
```

Renaming the multivariate model:

```{r}
Multivariate_mtcars_lm <- New_mtcars_lm
summary(Multivariate_mtcars_lm)
```

What does the linear function equation of a trivariate regression model look like ?:

```{r}
preview_eq(Multivariate_mtcars_lm)
```

After some exploratory analysis of our trivariate model it was observed that ,when the same trivariate model was transformed into a polynomial trivariate model (mpg\~wt^2^ + hp^2^ + cyl^2^ )

the R\^2 dropped and S.E inflated.

we therefore should restrict our multivariate model to only two predictor variables (wt and hp) in their 2nd degree polynomial form. Note our model still performs comparatively better than the previous models.

```{r}
Poly_multivariate_mtcars_lm <- lm(mpg~poly(wt,2)+poly(hp,2),mtcars) # Two of the three varibales fit better in the model with their second degree exponents (the adjusted R^2 for this model is 0.8745 and a has low 0.3774). The cyl variable was removed because it brought the R^2 down and increased the S.E.
summary(Poly_multivariate_mtcars_lm)
```

As more predictor variables are added into the model, the linear function also becomes more complex.

```{r}
preview_eq(Poly_multivariate_mtcars_lm)
```

Let's plot the 4 main diagnostic plots with this second-degree-multivariate-linear-regression model, to asses its accuracy and predicatibility.

```{r}
plot(Poly_multivariate_mtcars_lm)
```

The diagnostic plots reveal that our model fits much better now, however some non-linearity still lingers.

Since, the model is statistically sufficiently accurate now, let's use this model to predict the milage(mpg) of cars from a given(test) set of values and see the output.

The wt and hp values in the 'TEST_wt_values' list vector are made-up, just to test the model. **Infact the whole purpose of a regression model is to predict the response variable values based off of real world values,which are often arbitrary and different than the training data that was used to construct the model.**

The `predict()` function is used to read and feed these test values into our model, using its

'newdata=' parameter.

```{r}
TEST_wt_values <- list(wt=c(2.4,3.6,1.5),hp=c(114,230,200))
predict(Poly_multivariate_mtcars_lm, newdata = TEST_wt_values)
```

Now let's visualise the quadratic(2nd degree) relationship of wt and hp variables with the mpg variable, using simple scatterplot graphs. Since both wt and hp variables are in different ranges, it is important to facet the graph into two different juxtaposed graphs, each showing the relationship (regression curve) of either wt^2^ or hp\^2 at separate x-axes against mpg at the comon y-axis.

```{r}
library(patchwork) # For combining plots

# 1. Curve for Weight (holding hp constant)
plot_wt <- ggplot(mtcars, aes(wt, mpg)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), color = "blue") +
  labs(title = "Effect of Weight (poly 2)", subtitle = "Holding HP constant") +
  theme_minimal()

# 2. Curve for Horsepower (holding wt constant)
plot_hp <- ggplot(mtcars, aes(hp, mpg)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), color = "red") +
  labs(title = "Effect of Horsepower (poly 2)", subtitle = "Holding Weight constant") +
  theme_minimal()

# Combine them
plot_wt + plot_hp

```

Since our model has two continuous predictors, the "curves" actually form a curved 3D surface. We can visualize this using the plotly package.

```{r}
library(plotly)

# Create a grid of values for wt and hp
wt_seq <- seq(min(mtcars$wt), max(mtcars$wt), length.out = 30)
hp_seq <- seq(min(mtcars$hp), max(mtcars$hp), length.out = 30)
grid <- expand.grid(wt = wt_seq, hp = hp_seq)

# Predict mpg for the grid using your model
grid$mpg <- predict(Poly_multivariate_mtcars_lm, newdata = grid)

# Reshape for 3D plotting
z_matrix <- matrix(grid$mpg, nrow = 30, ncol = 30)

# Generate interactive 3D plot
plot_ly(x = ~hp_seq, y = ~wt_seq, z = ~z_matrix) %>% 
  add_surface() %>%
  layout(scene = list(xaxis = list(title = 'Horsepower'),
                      yaxis = list(title = 'Weight'),
                      zaxis = list(title = 'MPG')))

```

In reality the milage of a car is determined by many variables such as wt, cyl, hp, disp ,carb,etc. all of which have great degree of correlation with milage (either positive or negative), this phenomena is called **Multicolinearity** in statistics.

While making a regression model it is sometimes important to asses the effect (relation) of only one of these variables on our response variable.This can be achieved by making partial regression plots. PR plots filter out multicolinearity (by statistically muting the effects of all other variables) to show the relationship of a chosen/desired predictor variable with the response variable.

**Partial regression plot**, a.k.a **Added-variable plots** ,help us finding out the contribution of each predictor variable individually to the overall accuracy of the multivariate model.

A PR-plot for a multivariate model is a graph between the residuals of the response variable (adjusted for all other variables) and the residuals of each predictor variable included in the model (Separately-that are also adjusted for all other variables).

In Partial regression plots the regression line displays the **effect of** **only one predictor variable at a time against the response variable**. A visual comparison of these graphs of a multivariate model can reveal to what extent does a predictor variable alone contributes in the overall accuracy of the model:

`car::avPlots()` function can plot all the PR-plots of a multivariate model.

```{r}

library(car)
avPlots(lm(mpg~poly(wt,2)+poly(hp,2),mtcars))
```

The PR-curves above indicate that both wt and hp are negatively correlated with mpg (downward slopes in both the left graphs) i.e as weight or horsepower increase milage will decrease, however this relationship is not linear but curvilinear (quadratic here) and the resulting regression line for wt^2^ and/or hp\^2 will be curved (u-shaped) based on the ranges of both variables. This is represented by the positive slope in the two right hand side graphs, which suggests that although milage will decrease with increase in wt/hp, the rate of decrease will be diffferent as we move towards the heavier vehicle (or with higher horsepower) mentioned in this dataset (this trend may not hold true in real life).

PR-graphs for a polynomial trivariate model (wt+hp+cyl). The following graphs explain why cyl^2^ led to a decrease in R-squared value- i.e it does not adapt to the u-shaped curve as second degree polynomial variable.It also does not seem to correlate well with mpg linearly:

```{r}
avPlots(lm(mpg~poly(wt,2)+poly(hp,2)+poly(cyl,2),mtcars))
```

Let's try to make another polynomial trivariate model using 'disp' instead of 'cyl'.

It turns out that, including disp^3^ increased the adj R^2,^ indicating an S-shaped inflection point in the regression curve of mpg\~disp\^3.Moreover Increasing the exponent to 5th degree further increased the adjusted R-value (while maintaining a low S.E) ,this shows how complex the curvilinear trend is, in this dataset.

```{r}
summary(lm(mpg~poly(wt,2)+poly(hp,2)+poly(disp,5),mtcars))
```

```{r}
avPlots(lm(mpg~poly(wt,2)+poly(hp,2)+poly(disp,5),mtcars))
```

The complex curvilinear relationship between mpg and disp\^5 can be visualized by a simple scatter plot.

```{r}
mpg_to_disp_fitted <- fitted(lm(mpg~poly(disp,5),mtcars))
ggplot(mtcars,aes(disp,mpg))+
  geom_point()+
  geom_smooth(method= 'lm', formula= y~poly(x,5), se = TRUE)+
  theme_classic()+
geom_segment(aes(yend= mpg_to_disp_fitted))
```

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì‚Äî‚ÄîTHE END------------------------------------
